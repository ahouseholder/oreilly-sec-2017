{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering Example\n",
    "In this example notebook, you will see how to implement K-Means Clustering in Python using Scikit-Learn and Pandas. \n",
    "Adapted from https://pythonprogramming.net/flat-clustering-machine-learning-python-scikit-learn/\n",
    "\n",
    "## Step 1:  Get Data:\n",
    "The first step is to prepare or generate the data.  In this dataset, the observations only have two features, but K-Means can be used with any number of features.  Since this is an unsupervised example, it is not necessary to have a \"target\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     x     y\n",
      "0  1.0   2.0\n",
      "1  5.0   8.0\n",
      "2  1.5   1.8\n",
      "3  8.0   8.0\n",
      "4  1.0   0.6\n",
      "5  9.0  11.0\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame([[1, 2],\n",
    "              [5, 8],\n",
    "              [1.5, 1.8],\n",
    "              [8, 8],\n",
    "              [1, 0.6],\n",
    "              [9, 11]], columns=['x','y'])\n",
    "print( data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Build the Model:\n",
    "Much like the supervised models, you first create the model then call the `.fit()` method using your data source.  The model is now populated with both your centroids and labels.  These can be accessed via the `.cluster_centers_` and `labels_` properties respectively.\n",
    "\n",
    "You can view the complete documentation here: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "K-Means also has a `.predict()` method which can be used to predict the label for an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.33333333  9.        ]\n",
      " [ 1.16666667  1.46666667]]\n",
      "[1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=2).fit(data)\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "print(centroids)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Clusters\n",
    "The code below visualizes the clusters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAELCAYAAAA1AlaNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhr4kJcTQl6SEGPqSlBBDX5IS8v8BOW84iv0ePJ4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109c69898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['labels'] = labels\n",
    "\n",
    "#plt.plot(data, colors[data['labels'], markersize = 10)\n",
    "\n",
    "group1 = data[data['labels']==1].plot( kind='scatter', x='x', y='y',  s=100, color='DarkGreen', label=\"Group 1\" )\n",
    "group2 = data[data['labels']==0].plot( kind='scatter', x='x', y='y', s=100,color='Brown', ax=group1, label=\"Group 2\" )\n",
    "group1.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=3, fancybox=True, shadow=True)\n",
    "plt.scatter(centroids[:, 0],centroids[:, 1], marker = \"x\", s=150, linewidths = 5, zorder = 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with Real Data\n",
    "Now that you've tried K-means on some generated data, let's try it on some real data and see what we can produce. As before the first step is to read in the data into a DataFrame.  \n",
    "\n",
    "We will be using this data later, but the dataset consists of approximately 6000 domains--5000 of which were generated by various botnets and 1000 are from the Alexa 1 Million.  The columns are:\n",
    "\n",
    "* `dsrc`:  The source of the domain\n",
    "* `domain`:  The actual domain\n",
    "* `length`:  The length of the domain\n",
    "* `dicts`:  Percentage containing dictionary words\n",
    "* `entropy`:  The entropy of the domain\n",
    "* `numbers`:  The number of digits in the domain\n",
    "* `ngram`:  Different n-grams which appear in the domain (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dsrc</th>\n",
       "      <th>domain</th>\n",
       "      <th>length</th>\n",
       "      <th>dicts</th>\n",
       "      <th>entropy</th>\n",
       "      <th>numbers</th>\n",
       "      <th>ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51786</th>\n",
       "      <td>alexa</td>\n",
       "      <td>ru-an</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.321928</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34004</th>\n",
       "      <td>nivdort</td>\n",
       "      <td>offerhunt</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.947703</td>\n",
       "      <td>0</td>\n",
       "      <td>7.492816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>bamital</td>\n",
       "      <td>84d40603b85cb7f757e7171e92af2b1f</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.816428</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50037</th>\n",
       "      <td>alexa</td>\n",
       "      <td>lokeshdhakar</td>\n",
       "      <td>12</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>3.084963</td>\n",
       "      <td>0</td>\n",
       "      <td>6.312611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12746</th>\n",
       "      <td>gameoverdga</td>\n",
       "      <td>g3qnc5x4b0m63c7x69vfzzkh</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.168296</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dsrc                            domain  length     dicts  \\\n",
       "51786        alexa                             ru-an       5  0.000000   \n",
       "34004      nivdort                         offerhunt       9  1.000000   \n",
       "494        bamital  84d40603b85cb7f757e7171e92af2b1f      32  0.000000   \n",
       "50037        alexa                      lokeshdhakar      12  0.916667   \n",
       "12746  gameoverdga          g3qnc5x4b0m63c7x69vfzzkh      24  0.000000   \n",
       "\n",
       "        entropy  numbers     ngram  \n",
       "51786  2.321928        0  0.000000  \n",
       "34004  2.947703        0  7.492816  \n",
       "494    3.816428       21  0.000000  \n",
       "50037  3.084963        0  6.312611  \n",
       "12746  4.168296        9  0.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../../Data/dga-full.csv')\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Data\n",
    "Since clustering relies on measuring distances between objects it is important that all data points be on the same scale.  There are various methods for doing this, which are beyond the scope of this class, however, for this example, we will use scikit-learn's `StandardScaler` to accomplish this.  (http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "The StandardScaler transforms each column by:\n",
    "* Subtracting from the element in each row the mean for each feature (column) and then taking this value and\n",
    "* Dividing by that feature's (column's) standard deviation.\n",
    "\n",
    "Scikit-learn has a transformer interface which is very similar to the other scikit-learn interfaces.  The basic steps are:\n",
    "1.  Create the Scaler object\n",
    "2.  Using the feature matrix, call the `.fit()` method to \"train\" the Scaler\n",
    "3.  Use the `.transform()` method to scale the data.\n",
    "\n",
    "**NOTE**: When using a Scaler, it is important to train the scaler on your data, and use this trained scalers on any future predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_columns = ['length', 'dicts','entropy','numbers','ngram']\n",
    "scaled_feature_columns = ['scaled_length', 'scaled_dicts','scaled_entropy','scaled_numbers','scaled_ngram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step 1:  Create the scaler\n",
    "\n",
    "#Steps 2 & 3:  Fit the scaler and transform this data\n",
    "\n",
    "#Put the scaled data into a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data and you'll see that the data is now all scaled consistently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally for convenience, we're going to merge the scaled data with the non-scaled data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_data = pd.merge( data, scaled_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn!\n",
    "Now that we have data that is suitable (maybe) for clustering, in the section below, perform K-Means clustering on this data set.  Initially, start out with 2 clusters and assign the cluster id as a column in your DataFrame.\n",
    "\n",
    "Then do a `value_counts()` on the `dsrc` column for each cluster to see how the model divided the data.  Try various values for `k` to see how it performed.\n",
    "\n",
    "Remember to use the scaled features ONLY for your clustering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualizing Performance\n",
    "As we already know, it is difficult to measure the performance of clustering models since there usually is no known ground truth from which to evaluate your model.  However, there are two techniques which \n",
    "\n",
    "The K-Elbow Visualizer implements the “elbow” method of selecting the optimal number of clusters for K-means clustering. K-means is a simple unsupervised machine learning algorithm that groups data into a specified number (k) of clusters. Because the user must specify in advance what k to choose, the algorithm is somewhat naive – it assigns all members to k clusters even if that is not the right k for the dataset.\n",
    "\n",
    "The elbow method runs k-means clustering on the dataset for a range of values for k (say from 1-10) and then for each value of k computes an average score for all clusters. By default, the distortion_score is computed, the sum of square distances from each point to its assigned center. Other metrics can also be used such as the silhouette_score, the mean silhouette coefficient for all samples or the calinski_harabaz_score, which computes the ratio of dispersion between and within clusters.\n",
    "\n",
    "When these overall metrics for each model are plotted, it is possible to visually determine the best value for K. If the line chart looks like an arm, then the “elbow” (the point of inflection on the curve) is the best value of k. The “arm” can be either up or down, but if there is a strong inflection point, it is a good indication that the underlying model fits best at that point. (http://www.scikit-yb.org/en/latest/api/cluster/elbow.html)\n",
    "\n",
    "In python there is a module called `YellowBrick` which facilitates visualizing the K-Elbow score.  All of YellowBrick's visualizations follow essentually the same pattern:\n",
    "\n",
    "1.  Create the Visualizer Object\n",
    "2.  Call the `.fit()` method using the data\n",
    "3.  Call the `.poof()` method to render the visualization\n",
    "\n",
    "The snippet below demonstrates how to use the elbow method to visualize the clustering model's performance on this dataset.\n",
    "```python\n",
    "visualizer = KElbowVisualizer(KMeans(), k=(4,12))\n",
    "\n",
    "visualizer.fit( feature_matrix ) \n",
    "visualizer.poof()\n",
    "```\n",
    "\n",
    "### Your Turn!\n",
    "In the box below, create a visualization using the elbow method to see if there are any inflection points in the distortion score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Way to Visualize Clustering Performance\n",
    "The Silhouette Coefficient is used when the ground-truth about the dataset is unknown and computes the density of clusters computed by the model. The score is computed by averaging the silhouette coefficient for each sample, computed as the difference between the average intra-cluster distance and the mean nearest-cluster distance for each sample, normalized by the maximum value. This produces a score between 1 and -1, where 1 is highly dense clusters and -1 is completely incorrect clustering. (http://www.scikit-yb.org/en/latest/api/cluster/silhouette.html)\n",
    "\n",
    "\n",
    "### Your Turn!\n",
    "Using the YellowBrick `SilhouetteVisualizer`, try visualizing models with various values of `K`.\n",
    "\n",
    "**Note**:  This visualization is quite expensive, so I recommend performing this using a sample o your original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
