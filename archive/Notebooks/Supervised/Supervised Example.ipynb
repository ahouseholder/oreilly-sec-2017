{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning Examples\n",
    "Some examples of supervised machine learning examples in Python.\n",
    "First, load up a ton of modules..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from yellowbrick.classifier import ClassificationReport, ConfusionMatrix\n",
    "from sklearn import metrics\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load the data\n",
    "Next, we have to load the data into a dataframe.  In order to have a balanced dataset, we will use 10000 records from Alexa which will represent the not malicious domains, and 10000 records from `gameoverdga` representing the malicious domains.  \n",
    "\n",
    "You can see that at the end we have 10000 of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv( '../../data/dga-full.csv' )\n",
    "#Filter to alexo and game over\n",
    "df = df[df['dsrc'].isin(['alexa','gameoverdga'])]\n",
    "df.dsrc.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a Target Column\n",
    "For our datasets, we need a numeric column to represent the classes.  In our case we are going to call the column `isMalicious` and assign it a value of `0` if it is not malicious and `1` if it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['isMalicious'] = df['dsrc'].apply( lambda x: 0 if x == \"alexa\" else 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isMalicious'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the Train/Test Split\n",
    "For this, letâ€™s create a rather small training data se as it will reduce the time to train up a model.\n",
    "Feel free to try a 15%, 20% or even a 30% portion for the training data (lower percentages for slower machines).\n",
    "\n",
    "In this example, we will split 30% for train and 70% for test.\n",
    "\n",
    "Normally you would want most of the data in the training data, but more training data can considerably extend the time neede to train up a model.\n",
    "\n",
    "We're also going to need a list of column names for the feature columns as well as the target column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train, test = train_test_split(df, test_size = 0.7)\n",
    "features = ['length', 'dicts', 'entropy','numbers', 'ngram']\n",
    "target = 'isMalicious'\n",
    "\n",
    "feature_matrix = df[ features ]\n",
    "target_vector = df[ target ]\n",
    "\n",
    "#Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Classifiers\n",
    "The next step is to create the classifiers. What you'll see is that scikit-learn maintains a constant interface for every machine learning algorithm.  For a supervised model, the steps are:\n",
    "1.  Create the classifier object\n",
    "2.  Call the `.fit()` method with the training data set and the target \n",
    "3.  To make a prediction, call the `.predict()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the Random Forest Classifier\n",
    "random_forest_clf = # Your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Next, create the SVM classifier\n",
    "svm_classifier = # Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Classifiers\n",
    "Now that we have two different classifiers, let's compare them and see how they perform. Fortunately, Scikit has a series of functions to generate metrics for you.  The first is the cross validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = # Your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = # Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to to get the predictions from both classifiers, so we add columns to the test and training sets for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_test = random_forest_clf.predict( feature_matrix_test )\n",
    "predictions_train = random_forest_clf.predict( feature_matrix_train )\n",
    "svm_predictions_test = svm_classifier.predict( feature_matrix_test)\n",
    "svm_predictions_train = svm_classifier.predict( feature_matrix_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "These are a little confusing (yuk yuk), but are a very valuable tool in evaluating your models.  Scikit-learn has a function to generate a confusion matrix as shown below.  \n",
    "``` python\n",
    "confusion_matrix( target_test, predictions_test)\n",
    "```\n",
    "Try this yourself to see what the confusion matrices look like for various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try using YellowBrick to produce nicer, color coded confusion matrices.  Remember the syntax is:\n",
    "\n",
    "```python\n",
    "viz = ConfusionMatrix(RandomForestClassifier(), classes=[0,1])\n",
    "\n",
    "viz.fit(feature_matrix_train, target_train)  \n",
    "random_forest_visualizer.score(feature_matrix_test, target_test)  \n",
    "g = viz.poof()    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again for the SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate precision and recall for both models\n",
    "Next, you are going to want to compare the models performance metrics.  While scikit-learn does provide all the scores as part of the metrics package, it is easier to calculate all the metrics at once using the classification report functionality.  The basic syntax is:\n",
    "\n",
    "```python\n",
    "classification_report(y_true, y_pred, target_names=target_names))\n",
    "```\n",
    "YellowBrick also has a nice classification report visualizer.  The basic syntax is below:\n",
    "```python\n",
    "visualizer = ClassificationReport(model, classes=classes)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()         \n",
    "```\n",
    "\n",
    "Do this for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "Random Forest has a feature which can calculate the importance for each feature it uses in building the forest.  This can be calculated with  this property:`random_forest_clf.feature_importances_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also visualize this with the following code from: #From: http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.std([random_forest_clf.feature_importances_ for tree in random_forest_clf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(feature_matrix_test.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(feature_matrix_test.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(feature_matrix_test.shape[1]), indices)\n",
    "plt.xlim([-1, feature_matrix_test.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can calculate the accuracy with the `metrics.accuracy()` method, and finally, there is the `metrics.classification-report()` which will calculate all the metrics except accuracy at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pscore = metrics.accuracy_score(target_test, predictions_test)\n",
    "pscore_train = metrics.accuracy_score(target_train, predictions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( metrics.classification_report(target_test, predictions_test, target_names=['Malicious', 'Not Malicious'] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pscore = metrics.accuracy_score(target_test, svm_predictions_test)\n",
    "svm_pscore_train = metrics.accuracy_score(target_train, svm_predictions_train)\n",
    "print( metrics.classification_report(target_test, svm_predictions_test, target_names=['Malicious', 'Not Malicious'] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( svm_pscore, svm_pscore_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( pscore, pscore_train)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
